---
title: "Week Two Exercises"
author: "Chidi Justice"
date: "4/28/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
```

```{r echo=FALSE, message=FALSE}
library(downloader)
library(tidyverse)
library(gapminder)
library(rafalib)
library(patchwork)

url <- "https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/mice_pheno.csv"
mice_pheno <- paste0("data/", basename(url))
download(url, destfile = mice_pheno, quiet = TRUE)

population <- read.csv("data/femaleControlsPopulation.csv")$Bodyweight
femaleWeights <- read_csv("data/femaleMiceWeights.csv")
mice_pheno <- read.csv(mice_pheno)
```

###### Data used in exercise has already been downloaded [here](week2Note.Rmd)  

##### Q1: What is the average of these weights?  
```{r}
mean(population) # 23.9 grams
```

##### Q2: After setting the seed at 1, set.seed(1) take a random sample of size 5. What is the absolute value (use abs) of the difference between the average of the sample and the average of all the values?  
```{r}
set.seed(1)
abs(mean(sample(population, 5)) - mean(population))
```

##### Q3: After setting the seed at 5, set.seed(5) take a random sample of size 5. What is the absolute value of the difference between the average of the sample and the average of all the values?  
```{r}
set.seed(5)
abs(mean(sample(population, 5)) - mean(population))
```

##### Q4: Set the seed at 1, then using a for-loop take a random sample of 5 mice 1,000 times. Save these averages. What proportion of these 1,000 averages are more than 1 gram away from the average of x ?  
```{r}
set.seed(1)
sample_5 <- sapply(1:1000, function(x) {
  mean(sample(population, 5))
})
hist(sample_5)
mean(abs(sample_5 - mean(population)) > 1)
```

##### Q5: We are now going to increase the number of times we redo the sample from 1,000 to 10,000. Set the seed at 1, then using a for-loop take a random sample of 5 mice 10,000 times. Save these averages. What proportion of these 10,000 averages are more than 1 gram away from the average of x ?  
```{r}
set.seed(1)
sample_n10000 <- sapply(1:10000, function(x) {
  mean(sample(population, 5))
})
hist(sample_n10000)
mean(abs(sample_n10000 - mean(population)) > 1)
```
##### Q6: Note that the answers to 1 and 2 barely changed. This is expected. The way we think about the random value distributions is as the distribution of the list of values obtained if we repeated the experiment an infinite number of times. On a computer, we can't perform an infinite number of iterations so instead, for our examples, we consider 1,000 to be large enough, thus 10,000 is as well. Now if instead we change the sample size, then we change the random variable and thus its distribution.

Set the seed at 1, then using a for-loop take a random sample of 50 mice 1,000 times. Save these averages. What proportion of these 1,000 averages are more than 1 gram away from the average of x ?  
```{r}
set.seed(1)
sample_50 <- sapply(1:1000, function(x) {
  mean(sample(population, 50))
})
hist(sample_50)
mean(abs(sample_50 - mean(population)) > 1)
```

From the above, I see that it's rarer to find a difference of more than one grams from the average of the population compared to when the number of samples is smaller? Why is this so? I think its because as the sample size increases, the number of common values are represented more, and those rarer ones are sampled less in the sample, so as the size grows, it shows a better representation of the population.

```{r}
data("gapminder")
head(gapminder)

year1952 <- gapminder %>%
  filter(year == 1952)

ggplot(year1952, aes(x = lifeExp)) +
  geom_histogram(aes(fill = continent), binwidth = 1)
```

##### Q7: In statistics, the empirical cumulative distribution function (or empirical cdf or empirical distribution function) is the function F(a) for any a, which tells you the proportion of the values which are less than or equal to a.

We can compute F in two ways: the simplest way is to type `mean(x <= a)`. This calculates the number of values in x which are less than or equal a, divided by the total number of values in x, in other words the proportion of values less than or equal to a.

The second way, which is a bit more complex for beginners, is to use the ecdf() function. This is a bit complicated because this is a function that doesn't return a value, but a function.

Let's continue, using the simpler, mean() function.

What is the proportion of countries in 1952 that have a life expectancy less than or equal to 40?  
```{r}
year1952 %>%
  summarise(over40 = mean(lifeExp <= 40))
```

##### Q8: What is the proportion of countries in 1952 that have a life expectancy between 40 and 60 years? Hint: this is the proportion that have a life expectancy less than or equal to 60 years, minus the proportion that have a life expectancy less than or equal to 40 years.  
```{r}
year1952 %>%
  summarise(between40_and_60 = mean(lifeExp >= 40 & lifeExp <= 60))
```

To see the ECDF of the height distribution  
```{r}
plot(ecdf(year1952$lifeExp))
```

I can redo the plot above with the following pattern  

```{r}
proportions <- sapply(year1952$lifeExp, function(x) {
  mean(year1952$lifeExp <= x)
})
plot(year1952$lifeExp, proportions)
```

##### Q9: Use a histogram to "look" at the distribution of averages we get with a sample size of 5 and a sample size of 50. How would you say they differ?  
```{r}
mypar(1,2)
hist(sample_5)
hist(sample_50)
mypar(1,1)
qqplot(sample_5, sample_50)
```

##### Q10: For the last set of averages, the ones obtained from a sample size of 50, what proportion are between 23 and 25?  
```{r}
plot(ecdf(sample_50))
mean(sample_50 >= 23 & sample_50 <= 25)
```

##### Q11: Now ask the same question of a normal distribution with average 23.9 and standard deviation 0.43.  
```{r}
pnorm((25 - 23.9) / 0.43)  - pnorm((23 - 23.9) / 0.43) 
```
We will remove the lines that contain missing values:   
```{r}
mice_pheno_no_na = na.omit(mice_pheno)
head(mice_pheno_no_na)
```

##### Q12: Use dplyr to create a vector x with the body weight of all males on the control (chow) diet. What is this population's average?  
```{r}
mice_pheno_no_na %>%
  filter(Sex == "M", Diet == "chow") -> x
mean(x$Bodyweight)
```
##### Q13: Now use the rafalib package and use the popsd function to compute the population standard deviation.  
```{r}
popsd(x$Bodyweight)
```
##### Q14: Set the seed at 1. Take a random sample  of size 25 from x. What is the sample average?  
```{r}
set.seed(1)
x_sample <- sample(x$Bodyweight, 25)
mean(x_sample)
```
##### Q15: Use dplyr to create a vector y with the body weight of all males on the high fat hf diet. What is this population's average?  
```{r}
mice_pheno_no_na %>%
  filter(Sex == "M", Diet == "hf") -> y
mean(y$Bodyweight)
```
##### Q16: Now use the rafalib package and use the popsd function to compute the population standard deviation.  
```{r}
popsd(y$Bodyweight)
```
##### Q17: Set the seed at 1. Take a random sample  of size 25 from y. What is the sample average?  
```{r}
set.seed(1)
y_sample <- sample(y$Bodyweight, 25)
mean(y_sample)
```
##### Q18: What is the difference in absolute value between sample averages and population averages?  
```{r}
abs((mean(y_sample) - mean(x_sample)) - (mean(y$Bodyweight) - mean(x$Bodyweight)))
```
##### Q19: Repeat the above for females. Make sure to set the seed to 1 before each sample call. What is the difference in absolute value between sample averages and population averages?  
```{r}
mice_pheno_no_na %>%
  filter(Sex == "F", Diet == "hf") -> females_hf

mice_pheno_no_na %>%
  filter(Sex == "F", Diet == "chow") -> females_chow

set.seed(1)
samples_female_chow <- sample(females_chow$Bodyweight, 25)

set.seed(1)
samples_female_hf <- sample(females_hf$Bodyweight, 25)


abs((mean(samples_female_hf) - mean(samples_female_chow)) - (mean(females_hf$Bodyweight) - mean(females_chow$Bodyweight)))
```

##### Q20: For the females, our sample estimates were closer to the population difference than with males. What is a possible explanation for this?  
```{r}
mice_pheno_no_na %>%
  group_by(Sex) %>%
  summarise(averages = popsd(Bodyweight))
```
To check this, it is because the male sample has more variance than the female sample as seen above.  

##### Q21, Q22, Q23: If a list of numbers has a distribution that is well approximated by the normal distribution, what proportion of these numbers are within one standard deviation away from the list's average?  
~0.68 is 1 sd from the mean, 0.95 and 0.99 is respectively 2 and 2.5 standard deviations away from the mean  

##### Q24: Define y to be the weights of males on the control diet. What proportion of the mice are within one standard deviation away from the average weight (remember to use popsd for the population sd)?  
```{r}
mice_pheno %>%
  filter(Sex == "M", Diet == "chow", !is.na(Bodyweight)) %>%
  select(Bodyweight) %>%
  unlist() -> male_control

mean(male_control < (mean(male_control) + popsd(male_control)))
y <- mean(male_control)
x <- popsd(male_control)
mean(male_control < y + x) - mean(male_control < y - x)

# Provided solution, this is even more intuitive than the solution I presented above. First I calculate the distance from the mean
# for every value, then look for how many sd from the mean these differences represented by dividing by the population standard devition,
# then looking for the proportion less than or equal to 1 standard deviations in absolute value.
diff_from_mean <- (male_control - mean(male_control)) / popsd(male_control)
mean(abs(y) <= 1)
```

Although the above works, I got that solution from a student reply on the discussion board, honestly I don't understand why I had to subtract the proportion of mice less than the mean minus 1 standard deviation

##### Q25: What proportion of these numbers are within two standard deviations away from the list's average?    
```{r}
mean(male_control < y + (x * 2)) - mean(male_control < y - (x * 2))
```

##### Q26: What proportion of these numbers are within three standard deviations away from the list's average?  
```{r}
mean(male_control < y + (x * 3)) - mean(male_control < y - (x * 3))
```

##### Q26: Note that the numbers for the normal distribution and our weights are relatively close. Also, notice that we are indirectly comparing quantiles of the normal distribution to quantiles of the mouse weight distribution. We can actually compare all quantiles using a qqplot. Which of the following best describes the qq-plot comparing mouse weights to the normal distribution? 
```{r}
qqnorm(diff_from_mean)
qqline(diff_from_mean)
```

The above helps me compare if my distribution is well approximated by a normal distribution using a theoritical distribution, and the above shows that my distribution is well approximated by a normal distribution.  

##### Q27: Create the above qq-plot for the four populations: male/females on each of the two diets. What is the most likely explanation for the mouse weights being well approximated? What is the best explanation for all these being well approximated by the normal distribution?  
```{r}
plot_qqplot <- function(distribution) {
  qqnorm(distribution)
  qqline(distribution)
}

generate_data <- function(sex, diet) {
  return(mice_pheno_no_na %>%
    filter(Sex == sex, Diet == diet) %>% 
    select(Bodyweight) %>% 
    mutate(Bodyweight = ((Bodyweight - mean(Bodyweight)) / popsd(Bodyweight))) %>%
    unlist())
}

par(mfrow = c(2,2))

male_control <- generate_data("M", "chow")
male_hf <- generate_data("M", "hf")
female_hf <- generate_data("F", "hf")
female_control <- generate_data("F", "chow")

sapply(list(male_control, male_hf, female_control, female_hf), plot_qqplot)
```

##### Q28: Here we are going to use the function replicate to learn about the distribution of random variables. All the above exercises relate to the normal distribution as an approximation of the distribution of a fixed list of numbers or a population. We have not yet discussed probability in these exercises. If the distribution of a list of numbers is approximately normal, then if we pick a number at random from this distribution, it will follow a normal distribution. However, it is important to remember that stating that some quantity has a distribution does not necessarily imply this quantity is random. Also, keep in mind that this is not related to the central limit theorem. The central limit applies to averages of random variables. Let's explore this concept.

We will now take a sample of size 25 from the population of males on the chow diet. The average of this sample is our random variable. We will use the replicate to observe 10,000 realizations of this random variable. Set the seed at 1, generate these 10,000 averages. Make a histogram and qq-plot of these 10,000 numbers against the normal distribution.

We can see that, as predicted by the CLT, the distribution of the random variable is very well approximated by the normal distribution.  
```{r}
y <- filter(mice_pheno_no_na, Sex=="M" & Diet=="chow") %>% select(Bodyweight) %>% unlist
avgs <- replicate(10000, mean(sample(y, 25)))
mypar(1,2)
hist(avgs)
qqnorm(avgs)
qqline(avgs)

mean(avgs)
```

##### Q29: What is the standard deviation of the distribution of sample averages?  
```{r}
popsd(avgs)
```

##### Q30: The CLT is a result from probability theory. Much of probability theory was originally inspired by gambling. This theory is still used in practice by casinos. For example, they can estimate how many people need to play slots for there to be a 99.9999% probability of earning enough money to cover expenses. Let's try a simple example related to gambling.

##### Suppose we are interested in the proportion of times we see a 6 when rolling n = 100 die. This is a random variable which we can simulate with `x = sample(1:6, n, replace=TRUE)` and the proportion we are interested in can be expressed as an average: `mean( x == 6 )`. Because the die rolls are independent, the CLT applies.

##### We want to roll n dice 10,000 times and keep these proportions. This random variable (proportion of 6s) has mean `p=1/6` and variance `p\*(1-p)/n`. So according to CLT `z = (mean (x == 6) - p) / sqrt(p * (1-p) / n)` should be normal with mean 0 and SD 1. Set the seed to 1, then use `replicate` to perform the simulation, and report what proportion of times z was larger than 2 in absolute value (CLT says it should be about 0.05).  

```{r}
sampl <- function(p, n) {
  x = sample(1:6, n, replace = TRUE)
  return((mean(x == 6) - p) / q)
}

set.seed(1)

n <- 100
p = 1/6
q <- sqrt(p * (1 - p) / n)

z_proportion <- replicate(10000, sampl(p, n))
mean(abs(z_proportion) > 2)
```

##### Q31: For the last simulation you can make a qqplot to confirm the normal approximation. Now, the CLT is an asympototic result, meaning it is closer and closer to being a perfect approximation as the sample size increases. In practice, however, we need to decide if it is appropriate for actual sample sizes. Is 10 enough? 15? 30?

##### In the example used in exercise 1, the original data is binary (either 6 or not). In this case, the success probability also affects the appropriateness of the CLT. With very low probabilities, we need larger sample sizes for the CLT to "kick in".

##### Run the simulation from exercise 1, but for different values of p and n. For which of the following is the normal approximation best?  
```{r}
sampld <- function(p, n, sides) {
  x = sample(1:sides, n, replace = TRUE)
  return((mean(x == 1) - p) / sqrt(p * (1 - p) / n))
}

mypar(2, 2)

n_5 <- replicate(10000, sampld(0.5, 5, 1/0.5))
n_30 <- replicate(10000, sampld(0.5, 30, 1/0.5))
n_30_p001 <- replicate(10000, sampld(0.01, 30, 1/0.01))
n_100 <- replicate(10000, sampld(0.01, 100, 1/0.01))

mean(abs(n_5) > 2)
qqnorm(n_5, main = "Normal Q-Q Plot. N = 5, P = .5")
abline(0, 1) # if clt holds, we expect mean 0 and sd 1 which is the line abline should draw for this graph

qqnorm(n_30, main = "Normal Q-Q Plot. N = 30, P = .5")
abline(0, 1)

qqnorm(n_30_p001, main = "Normal Q-Q Plot. N = 30, P = .01")
abline(0, 1)

qqnorm(n_100, main = "Normal Q-Q Plot. N = 100, P = .01")
abline(0, 1)
```

##### Q32: As we have already seen, the CLT also applies to averages of quantitative data. A major difference with binary data, for which we know the variance is , is that with quantitative data we need to estimate the population standard deviation.

##### In several previous exercises we have illustrated statistical concepts with the unrealistic situation of having access to the entire population. In practice, we do **not** have access to entire populations. Instead, we obtain one random sample and need to reach conclusions analyzing that data. dat is an example of a typical simple dataset representing just one sample. We have 12 measurements for each of two populations:

```
    X <- filter(dat, Diet=="chow") %>% select(Bodyweight) %>% unlist
    Y <- filter(dat, Diet=="hf") %>% select(Bodyweight) %>% unlist
```

##### We think of $X$ as a random sample from the population of all mice in the control diet and $Y$ as a random sample from the population of all mice in the high fat diet.

##### Define the parameter $\mu x$ as the average of the control population. We estimate this parameter with the sample average $\overline{X}$. What is the sample average?  
```{r}
control <- filter(femaleWeights, Diet == "chow") %>% select(Bodyweight) %>% unlist
treatment <- filter(femaleWeights, Diet == "hf") %>% select(Bodyweight) %>% unlist
mean(control)
```

##### Q: Use the CLT to approximate the probability that our estimate  is off by more than 2 grams from . 

```{r}

```














